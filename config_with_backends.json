{
  "backends": {
    "ollama-local": {
      "type": "ollama",
      "enabled": true,
      "url": "http://localhost:11434",
      "timeout": 600,
      "connect_timeout": 45,
      "description": "Local Ollama instance"
    },
    "llama-cpp-local": {
      "type": "llama_cpp",
      "enabled": true,
      "url": "http://localhost:8120",
      "timeout": 600,
      "connect_timeout": 45,
      "api_key": null,
      "description": "Local llama.cpp server"
    },
    "ollama-remote": {
      "type": "ollama",
      "enabled": false,
      "url": "http://remote-server:11434",
      "timeout": 300,
      "connect_timeout": 30,
      "description": "Remote Ollama instance (disabled by default)"
    }
  },
  "timeouts": {
    "ollama_timeout": 600,
    "ollama_connect_timeout": 45,
    "llama_cpp_timeout": 600,
    "llama_cpp_connect_timeout": 45
  },
  "model_options": {
    "temperature": 0.1,
    "top_p": 0.95,
    "top_k": 50,
    "min_p": 0.01,
    "typical_p": 0.95,
    "num_predict": 4096,
    "num_ctx": 8192,
    "repeat_penalty": 1.15,
    "repeat_last_n": 64,
    "presence_penalty": 0.0,
    "frequency_penalty": 0.0,
    "penalize_newline": false,
    "stop": ["\n\nHuman:", "\n\nUser:"],
    "seed": null
  },
  "performance": {
    "context_history_limit": 15,
    "num_thread": -1,
    "num_gpu": 0,
    "main_gpu": 0,
    "num_batch": 1,
    "num_keep": 10,
    "use_mlock": true,
    "use_mmap": true,
    "numa": false
  },
  "system_prompt": "You are Dost, a knowledgeable and thoughtful AI assistant. Take time to provide detailed, accurate, and well-reasoned responses. Consider multiple perspectives and provide comprehensive information when helpful.",
  "response_optimization": {
    "stream": false,
    "keep_alive": "10m",
    "low_vram": false,
    "f16_kv": false,
    "logits_all": false,
    "vocab_only": false,
    "use_mmap": true,
    "use_mlock": false,
    "embedding_only": false,
    "numa": false
  },
  "_metadata": {
    "version": "1.2.0",
    "description": "Chat-o-llama configuration with backend abstraction support",
    "last_updated": "2024-12-19",
    "notes": [
      "This configuration supports multiple backends (Ollama and llama.cpp)",
      "Environment variables can override backend settings:",
      "  OLLAMA_URL, OLLAMA_TIMEOUT, OLLAMA_API_KEY",
      "  LLAMA_CPP_URL, LLAMA_CPP_TIMEOUT, LLAMA_CPP_API_KEY",
      "Backend-specific settings inherit from global model_options and performance",
      "Health monitoring runs automatically every 30 seconds",
      "Backends can be enabled/disabled individually"
    ]
  }
}